{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose and idea\n",
    "Understand how the RNN works and accurately document the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "First, I will generate words\n",
    "If it works, I'll try sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 'abcdefghigklmnopqrstuvwxyz '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(letter, vocab):\n",
    "    \"\"\"Return index of given letter in vocabulary.\"\"\"\n",
    "    try:\n",
    "        temp = vocab.index(letter)\n",
    "    except ValueError:\n",
    "        temp = -1\n",
    "    return temp\n",
    "\n",
    "\n",
    "def word_to_vector(word):\n",
    "    \"\"\"Return an array of vectors\n",
    "    of letters in one-hot representarion.\n",
    "    \"\"\"\n",
    "    vec = np.zeros((len(word), len(vocab), 1))\n",
    "    for i, letter in enumerate(word):\n",
    "        vec[i, find_index(letter, vocab)] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def vector_to_word(vec):\n",
    "    \"\"\"Inverse to word_to_vector.\n",
    "    Reruns a string of letters\n",
    "    \"\"\"\n",
    "    word = [None for _ in range(vec.shape[0])]\n",
    "    for i in range(vec.shape[0]):\n",
    "        temp = np.argmax(vec[i])\n",
    "        word[i] = vocab[temp] \n",
    "    return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reading(file_name):\n",
    "    \"\"\"Data reading and converting to vectorised form.\"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        read_data = file.readline()\n",
    "        read_data = read_data.lower()\n",
    "        while(read_data):\n",
    "            x.append(word_to_vector(read_data[:-1]))\n",
    "            y.append(word_to_vector(read_data[1:]))\n",
    "            read_data = file.readline()\n",
    "            read_data = read_data.lower()\n",
    "    data = list(zip(x, y))\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "DATA = data_reading('data/simple_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    a = np.exp(z)\n",
    "    return a / np.sum(a)\n",
    "\n",
    "class CrossEntropyCost():\n",
    "    \"\"\"\n",
    "    Crossentropy cost and derivative with sigmoidal activations.\n",
    "    \"\"\"\n",
    "\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a)))\n",
    "    \n",
    "    def delta(z, a, y):\n",
    "        return a - y\n",
    "    \n",
    "    \n",
    "class RNNetwork():\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, bptt_trunc=4, cost=CrossEntropyCost):\n",
    "        \"\"\"\n",
    "        hidden_size is a number of internal neurons,\n",
    "        bptt_trunc responsible for the number of BPTT's execution cycles,\n",
    "        cost is a cost-function of this model.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "        #weights and internal state initialisation\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size)\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.Wha = np.random.randn(vocab_size, hidden_size)\n",
    "        self.h = np.zeros((hidden_size, 1))\n",
    "        #self.bh = np.zeros((hidden_size, 1))\n",
    "        #self.ba = np.zeros((vocab_size, 1))\n",
    "        self.cost = cost\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"Return output of network as an array of vectors.\"\"\"\n",
    "        time_steps = len(x)\n",
    "        z = np.zeros((time_steps, self.vocab_size, 1))\n",
    "        #h = np.zeros((time_steps + 1, self.hidden_size, 1))\n",
    "        #h[-1] = np.zeros(h[0].shape)\n",
    "        for t in range(time_steps):\n",
    "            self.h = np.tanh(self.Wxh @ x[t] + self.Whh @ self.h)# + self.bh)\n",
    "            \"\"\"z[t] = self.Wha @ self.h #+ self.ba\n",
    "        return softmax(z)\"\"\"\n",
    "            z[t] = softmax(self.Wha @ self.h)\n",
    "        return z\n",
    "    \n",
    "    def feedforward_with_data(self, x):\n",
    "        \"\"\"\n",
    "        Return output of network as an array of vectors\n",
    "        and an array of internal states, in which the\n",
    "        network was in process.\n",
    "        \"\"\"\n",
    "        time_steps = len(x)\n",
    "        z = np.zeros((time_steps, self.vocab_size, 1))\n",
    "        h = np.zeros((time_steps + 1, self.hidden_size, 1))\n",
    "        h[-1] = np.zeros(h[0].shape)\n",
    "        for t in range(time_steps):\n",
    "            self.h = np.tanh(self.Wxh @ x[t] + self.Whh @ self.h)# + self.bh)\n",
    "            z[t] = softmax(self.Wha @ self.h) #+ self.ba\n",
    "            h[t] = self.h\n",
    "        return z, h\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"Returns loss on a single example w.r.t. cost-function\"\"\"\n",
    "        return self.cost.fn(self.feedforward(x), y) / len(x)\n",
    "    \n",
    "    def total_loss(self, data):\n",
    "        \"\"\"Returns loss on a data sample w.r.t. cost-function\"\"\"\n",
    "        n = len(data)\n",
    "        cost = 0.0\n",
    "        for (x, y) in data:\n",
    "            cost += self.loss(x, y)\n",
    "        return cost / n\n",
    "    \n",
    "    def BPTT(self, data, learning_rate=1):\n",
    "        \"\"\"\n",
    "        Training model using backpropogation through time algorithm\n",
    "        The data is a list of arrays (x, y).\n",
    "        \"\"\"\n",
    "        params = [self.Wxh, self.Whh, self.Wha]\n",
    "        for (x, y) in data:\n",
    "            nabla_params = self.backprop(x, y)\n",
    "            for (param, nabla_param) in zip(params, nabla_params):\n",
    "                param -= learning_rate * nabla_param\n",
    "                \n",
    "    def predict(self, initial, time_steps = 10):\n",
    "        vectorized = WordToAlphabet(initial)\n",
    "        for t in range(len(vectorized)):\n",
    "            self.h = np.tanh(self.Wxh @ vectorized[t] + self.Whh @ self.h)# + self.bh)\n",
    "        for t in range(time_steps):\n",
    "            self.h = np.tanh(self.Wxh @ x[t] + self.Whh @ self.h)# + self.bh)\n",
    "            z[t] = self.Wha @ self.h #+ self.ba\n",
    "            h[t] = self.h\n",
    "        return 0\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        time_steps = len(x)\n",
    "        nabla_Wxh = np.zeros(self.Wxh.shape)\n",
    "        nabla_Whh = np.zeros(self.Whh.shape)\n",
    "        nabla_Wha = np.zeros(self.Wha.shape)\n",
    "        y_hat, h = self.feedforward_with_data(x)\n",
    "        \n",
    "        delta = np.dot(self.Wha.T, y_hat[time_steps - 1] - y[time_steps - 1]) #delta = dl/dh\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            nabla_Wha += np.dot(y_hat[t] - y[t], h[t].T)\n",
    "        for t in range(time_steps - 2, -1, -1): #already have done the last time step\n",
    "            delta_help = np.dot(self.Wha.T, y_hat[t] - y[t])\n",
    "            helper = np.diag((np.ones(h[t].shape) - h[t] ** 2)[:, 0])\n",
    "            #print(helper.shape, delta.shape)\n",
    "            delta = np.dot(self.Whh.T, helper) @ delta + delta_help\n",
    "            nabla_Whh += np.dot(helper @ delta, h[t - 1].T)\n",
    "            nabla_Wxh += np.dot(helper @ delta, x[t - 1].T)\n",
    "        return [nabla_Wxh, nabla_Whh, nabla_Wha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNetwork(len(vocab), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.670568384313377"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.BPTT(DATA, 0.00005)\n",
    "net.total_loss(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'p', 'v', 'c', 'x', 'c', 'g'] ['r', 'o', 'g', 'r', 'a', 'm', ' ']\n",
      "['b', 'w', 'b', 'f', 'e', 'p', 'f', 'k', ' '] ['n', 'o', 'w', 'l', 'e', 'd', 'g', 'e', ' ']\n",
      "['b', 'x', 'x', 'f', 'i', 't', 'c', 'w', 'u', 'b'] ['n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ']\n",
      "[' ', 'q'] ['v', ' ']\n",
      "[' ', 'f', 'k', 'a', ' ', 't'] ['h', 'e', 'o', 'r', 'y', ' ']\n",
      "['d', 'x', 'c', 'w', 'b', ' ', 'y', 'o'] ['a', 'm', 'p', 'a', 'i', 'g', 'n', ' ']\n",
      "['t', 'd', 'd', 'h', ' ', 'p', 'h', 'p'] ['p', 'p', 'r', 'o', 'a', 'c', 'h', ' ']\n",
      "['t', ' '] ['o', ' ']\n",
      "['p', ' ', 'y', ' ', 'y', 'l', 'x'] ['e', 'c', 'e', 'i', 'v', 'e', ' ']\n",
      "['y', 'u', 'b', 'x'] ['o', 's', 't', ' ']\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,110):\n",
    "    out, h = net.feedforward_with_data(np.array(DATA[i][0]))\n",
    "    print(vector_to_word(out), vector_to_word(DATA[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока полный бардак, но есть план<br>\n",
    "Проверить правильность формул BPTT<br>\n",
    "Построить график loss(% complete)<br>\n",
    "Сменить базу данных<br>\n",
    "Добавить biases<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
