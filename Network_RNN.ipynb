{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = 'abcdefghigklmnopqrstuvwxyz '\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(letter, vocab):\n",
    "    try:\n",
    "        temp = vocab.index(letter)\n",
    "    except ValueError:\n",
    "        temp = -1\n",
    "    return temp\n",
    "\n",
    "def WordToAlphabet(word):\n",
    "    vec = np.zeros((len(word), len(vocab), 1))\n",
    "    for i, letter in enumerate(word):\n",
    "        vec[i, find(letter, vocab)] = 1.0\n",
    "    return vec\n",
    "\n",
    "def vecToWord(vec):\n",
    "    word = [None for _ in range(vec.shape[0])]\n",
    "    for i in range(vec.shape[0]):\n",
    "        temp = np.argmax(vec[i])\n",
    "        word[i] = vocab[temp]    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ability\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "with open('dataset/texts/simple_words.txt', 'r') as file:\n",
    "    read_data = file.readline()\n",
    "    read_data = read_data.lower()\n",
    "    print(read_data[:])\n",
    "    #lines = file.readlines()\n",
    "    while(read_data):\n",
    "        x.append(WordToAlphabet(read_data[:-1]))\n",
    "        y.append(WordToAlphabet(read_data[1:]))\n",
    "        read_data = file.readline()\n",
    "        read_data = read_data.lower()\n",
    "        \n",
    "data = list(zip(x, y))\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class ActivationSoftmax():\n",
    "    @staticmethod\n",
    "    def f(z):\n",
    "        a = np.exp(z)\n",
    "        return a / np.sum(a)\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "\"\"\"\n",
    "def softmax(z):\n",
    "    a = np.exp(z)\n",
    "    return a / np.sum(a)\n",
    "\n",
    "class CrossEntropyCost():\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)))\n",
    "    def delta(z, a, y):\n",
    "        return a - y\n",
    "    \n",
    "class RNNetwork():\n",
    "    def __init__(self, vocab_size, hidden_size, bptt_trunc = 4, cost = CrossEntropyCost):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size)\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.Wha = np.random.randn(vocab_size, hidden_size)\n",
    "        self.h = np.zeros((hidden_size, 1))\n",
    "        #self.bh = np.zeros((hidden_size, 1))\n",
    "        #self.ba = np.zeros((vocab_size, 1))\n",
    "        self.cost = cost\n",
    "        # U Wxh\n",
    "        # W Whh\n",
    "        # V Wha\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        time_steps = len(x)\n",
    "        z = np.zeros((time_steps, self.vocab_size, 1))\n",
    "        #h = np.zeros((time_steps + 1, self.hidden_size, 1))\n",
    "        #h[-1] = np.zeros(h[0].shape)\n",
    "        for t in range(time_steps):\n",
    "            self.h = np.tanh(self.Wxh @ x[t] + self.Whh @ self.h)# + self.bh)\n",
    "            \"\"\"z[t] = self.Wha @ self.h #+ self.ba\n",
    "        return softmax(z)\"\"\"\n",
    "    #why not softmax(z[t])\n",
    "            z[t] = softmax(self.Wha @ self.h)\n",
    "        return z\n",
    "    \n",
    "    def feedforwardWithData(self, x):\n",
    "        time_steps = len(x)\n",
    "        z = np.zeros((time_steps, self.vocab_size, 1))\n",
    "        h = np.zeros((time_steps + 1, self.hidden_size, 1))\n",
    "        h[-1] = np.zeros(h[0].shape)\n",
    "        for t in range(time_steps):\n",
    "            self.h = np.tanh(self.Wxh @ x[t] + self.Whh @ self.h)# + self.bh)\n",
    "            z[t] = softmax(self.Wha @ self.h) #+ self.ba\n",
    "            h[t] = self.h\n",
    "        return z, h\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        return self.cost.fn(self.feedforward(x), y) / len(x)\n",
    "    \n",
    "    def total_loss(self, data):\n",
    "        n = len(data)\n",
    "        cost = 0.0\n",
    "        for (x, y) in data:\n",
    "            cost += self.loss(x, y)\n",
    "        return cost / n\n",
    "    \n",
    "    def BPTT(self, data, learning_rate = 1):\n",
    "        params = [self.Wxh, self.Whh, self.Wha]\n",
    "        for (x, y) in data:\n",
    "            nabla_params = self.backprop(x, y)\n",
    "            for (param, nabla_param) in zip(params, nabla_params):\n",
    "                param -= learning_rate * nabla_param\n",
    "                \n",
    "    def predict(self, initial, time_steps = 10):\n",
    "        vectorized = WordToAlphabet(initial)\n",
    "        for t in range(len(vectorized)):\n",
    "            self.h = np.tanh(self.Wxh @ vectorized[t] + self.Whh @ self.h)# + self.bh)\n",
    "        for t in range(time_steps):\n",
    "            self.h = np.tanh(self.Wxh @ x[t] + self.Whh @ self.h)# + self.bh)\n",
    "            z[t] = self.Wha @ self.h #+ self.ba\n",
    "            h[t] = self.h\n",
    "        return 0\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        time_steps = len(x)\n",
    "        nabla_Wxh = np.zeros(self.Wxh.shape)\n",
    "        nabla_Whh = np.zeros(self.Whh.shape)\n",
    "        nabla_Wha = np.zeros(self.Wha.shape)\n",
    "        y_hat, h = self.feedforwardWithData(x)\n",
    "        \n",
    "        delta = np.dot(self.Wha.T, y_hat[time_steps - 1] - y[time_steps - 1]) #delta = dl/dh\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            nabla_Wha += np.dot(y_hat[t] - y[t], h[t].T)\n",
    "        for t in range(time_steps - 2, -1, -1): #already have done the last time step\n",
    "            delta_help = np.dot(self.Wha.T, y_hat[t] - y[t])\n",
    "            helper = np.diag((np.ones(h[t].shape) - h[t] ** 2)[:, 0])\n",
    "            #print(helper.shape, delta.shape)\n",
    "            delta = np.dot(self.Whh.T, helper) @ delta + delta_help\n",
    "            nabla_Whh += np.dot(helper @ delta, h[t - 1].T)\n",
    "            nabla_Wxh += np.dot(helper @ delta, x[t - 1].T)\n",
    "        return [nabla_Wxh, nabla_Whh, nabla_Wha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить правильность формул<br>\n",
    "Построить график loss(% complete)<br>\n",
    "Сменить базу данных<br>\n",
    "Добавить biases<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNetwork(len(vocab), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1244035740168687"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.BPTT(data, 0.005)\n",
    "net.total_loss(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['e', 'i', 'z', 'z', 'z', 'z', 'z', 'l', 'z', 'n', 'h'],\n",
       " ['e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', 'z'])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 14\n",
    "out, h = net.feedforwardWithData(np.array(data[i][0]))\n",
    "vecToWord(out), vecToWord(data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1186227957327524"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.total_loss(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
